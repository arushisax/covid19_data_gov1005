---
title: "Impact of COVID-19, Economies"
author: "Jun-Yong Kim"
date: "4/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(dplyr)
library(rvest)
library(gt)
library(janitor)
library(infer)
library(skimr)
library(tidycensus)
library(maps)
library(sf)
library(tibble)
library(stringr)
library(readxl)
library(date)
library(chron)
```

```{r covid-19, include = FALSE}

# Downloading country-level stats on cases of COVID-19 from Worldometers

# worldometer_url <- paste0("https://www.worldometers.info/coronavirus/")
# worldometer_html <- read_html(worldometer_url)
# worldometer <- worldometer_html %>% 
#   html_nodes("table")
# worldometer <- worldometer[[1]] %>% 
#  html_table
# worldometer <- worldometer %>% 
#   clean_names() %>% 
#  rename(country_name = "country_other")

# worldometer$total_cases <- gsub(',', '', worldometer$total_cases)

#apply repeats process for rows or columns. First argument is subject (so the
#data set that we are applying to). Second argument is row or column (1 for row,
#2 for column). Last argument is the process we want repeated: get rid of the
#comma first, then get rid of the + bc (it has a special place in regular
#expression). Escape it with backspace or double backspace in gsub to remove it.
#We want to apply changes to matrix, so we have to do some work to convert to
#dataframe, numerics, etc. 

# worldometer[, -1] --> if empty, pulls everything. comes after: selects
# columns, -1 means ignore 1st column

#total cases, deaths, etc scraped

#worldometer$total_cases <- gsub(',', '', worldometer$total_cases)

# worldometer <- apply(worldometer, 2, function(x){gsub(',|\\+', '', x)})
# worldometer <- as.data.frame(worldometer)
# worldometer[, -1] <- apply(worldometer[, -1], 2, as.numeric)

#apply repeats process for rows or columns 
#first arg is subject (so data, where you wanna apply)
#second argument is row or column (1 is row, 2 is column)
#last is process that you want repeated 
#get rid of the comma first
#then get rid of the + bc it has a special place in regular expression
#escape it with backspace, or double backspace in gsub to remove it 
#apply changes to matrix, so you have to do some work to convert to dataframe, numerics, etc 
#worldometer[, -1] --> if empty, pulls everything. comes after: selects columns, -1 means ignore 1st column 

# Reading in the cleaned, prepared data from team_data

worldometer <- readRDS("../team_data/worldometer.RDS")
covidGlobal <- readRDS("../team_data/covidGlobal.RDS")
covidUS <- readRDS("../team_data/covidUS.RDS")
nytimes_states <- readRDS("../team_data/nytimes_states.RDS")
```


```{r gdp_pop_data, include = FALSE}

# Get static World Bank population data, most recent as of 2018

population_data_18 <- read_csv("API_pop.csv", skip = 3) %>% 
  clean_names() %>% 
  select(country_name, x2018) %>% 
  rename(pop_2018 = x2018)

# Get static World Bank GDP data, most recent as of 2018
  
gdp_data_18 <- read_csv("API_gdp.csv", skip = 3) %>%
  clean_names() %>% 
  select(country_name, x2018) %>% 
  rename(gdp_2018 = x2018)

# Combine and create variable for GDP per capita

gdp_pop_2018 <- gdp_data_18 %>% 
  left_join(population_data_18, by = "country_name") %>% 
  mutate(gdp_per_capita = round(gdp_2018 / pop_2018, digits = 2))
```

```{r econ_indicators, include = FALSE}

#function to take stock indices from yahoo and scrape data every time its run (updated daily)

stock <- function(url) {
  stock_source <- paste0(url)
  stock_html <- read_html(stock_source)
  stock_data <- stock_html %>% 
  html_nodes("table")
stock_data <- stock_data[[1]] %>% 
  html_table
stock_data <- stock_data %>% 
  clean_names() %>% 
  select(date, close)
}

  
#korea

kospi <- stock("https://finance.yahoo.com/quote/%5EKS11/history?p=%5EKS11") %>% 
  rename(KOSPI = close)
kospi$date <- as.Date(kospi$date, format = "%B %d,%Y") 

#usa

nasdaq <- stock("https://finance.yahoo.com/quote/%5EIXIC/history?p=%5EIXIC") %>% 
  rename(NASDAQ = close)
nasdaq$date <- as.Date(nasdaq$date, format = "%B %d,%Y") 

#world

msci <- stock("https://finance.yahoo.com/quote/MSCI/history?p=MSCI") %>% 
  rename(MSCI = close)
msci$date <- as.Date(msci$date, format = "%B %d,%Y") 

#china

sse_china <- stock("https://finance.yahoo.com/quote/000001.SS/history?p=000001.SS") %>% 
  rename(SSE_China = close)
sse_china$date <- as.Date(sse_china$date, format = "%B %d,%Y") 

#europe as a whole 

stxe600_europe <- stock("https://finance.yahoo.com/quote/%5ESTOXX/history?p=%5ESTOXX") %>% 
  rename(stxe600_europe = close)
stxe600_europe$date <- as.Date(stxe600_europe$date, format = "%B %d,%Y") 

#italy

ftse_italy <- stock("https://finance.yahoo.com/quote/%5EFTSE%3FP%3DFTSE/history/") %>% 
  rename(ftse_italy = close)
ftse_italy$date <- as.Date(ftse_italy$date, format = "%B %d,%Y") 

#spain

ibex_spain <- stock("https://finance.yahoo.com/quote/%5EIBEX/history?p=%5EIBEX") %>% 
  rename(ibex_spain = close)
ibex_spain$date <- as.Date(ibex_spain$date, format = "%B %d,%Y") 

#willing to add more countries here. Perhaps France / Germany ? Iran? Singapore? 


```

```{r jhu, include = FALSE}

#jhu_confirmed_series <- read_csv("./GitHub/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv") %>% 
#  clean_names()

#tidy_jhu_confirmed <- jhu_confirmed_series %>%
 # pivot_longer(cols = c(x1_22_20:x4_13_20), names_to = "date", values_to = "confirmed") %>%
#  select(country_region, date, confirmed) %>%
#  group_by(date)

#tidy_jhu_confirmed$date <- as.Date(tidy_jhu_confirmed$date , format = "x%m_%d_%y") 

#tidy_jhu_confirmed %>%
 # skim()


#commenting out most code for now. Just if we are going to be using the rds data, then we should look to incorporate that (haven't looked into using it with rmarkdown yet) 

```


```{r combining, include= FALSE}

#tidy_jhu_us_confirmed <- jhu_us_confirmed_series %>%
#  pivot_longer(cols = c(x1_22_20:x4_2_20), names_to = "date", values_to = "confirmed") %>%
#  select(country_region, fips, combined_key, date, confirmed)

#tidy_jhu_us_confirmed %>%
 # group_by(combined_key, date) %>%
 # arrange(desc(confirmed))



#colnames(worldometer)[1] <- 'country_name'
#test2 <- worldometer[!(worldometer$country_name %in% test$country_name), ]

# shows entries worldometer data that was not matched 
# probably have to manually change 

#test <- merge.data.frame(gdp_pop_2018, worldometer, 'country_name', all = F)

tidy_gdp_pop <- gdp_pop_2018 %>% 
 left_join(worldometer, by = c("country_name" = "country_other")) %>% 
 select(country_name, pop_2018, gdp_2018, gdp_per_capita, total_cases, total_deaths, total_recovered) %>% 
 na.omit()



#total_daily_covid_deaths <- read_csv("./total-daily-covid-deaths.csv") %>%  
 # clean_names()
#total_daily_covid_deaths$date <- as.Date(total_daily_covid_deaths$date, format = "%B %d,%Y")


#ask for help 
#gotta figure out so I can do shiny

```

```{r plots, echo = FALSE}

#gets today's date for downloading file

date <- today <- format(Sys.time(), "%Y%m%d")

#crafting url by splitting up 
citymapper_url <- "https://cdn.citymapper.com/data/cmi/Citymapper_Mobility_Index_"

#pasting together to piece it together 
citymapper_url <- paste(citymapper_url, date, ".csv")

#downloading file from internet
citymapper <- download.file(citymapper_url, destfile = "./citymapper.csv", quiet = TRUE)

#creating citymapper database
citymapper <- read_csv("./citymapper.csv", skip = 3)


  cases_gdp_capita <- tidy_gdp_pop %>% 
  filter(country_name != "World") %>% 
  ggplot(aes(x = log(gdp_per_capita), y = log(total_cases), fill = total_deaths)) + 
  geom_point() 

#log scale accounts for outliers 

cases_gdp_capita
# plot is coming out really whack.will ask June
#geom_sf? 

deaths_gdp_capita <- tidy_gdp_pop %>% 
  ggplot(aes(x = log(total_deaths), y = log(gdp_per_capita), fill = total_deaths)) + 
  geom_point()

deaths_gdp_capita
```


